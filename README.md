


# üß† NeuroOptima Optimizer 


# EN version of Readme


**NeuroOptima** is a cutting-edge hybrid optimizer designed to enhance the training of deep learning models. Developed by **NeuroGhost**, this optimizer amalgamates the strengths of several advanced optimization techniques to achieve superior convergence speed, stability, and generalization performance.

 Below are Mermaid diagrams that compare the performance of **NeuroOptima** with traditional optimizers under various training conditions. These visualizations can be integrated into your GitHub repository to illustrate the advantages of NeuroOptima.

---

## üìä Comparative Performance Diagrams

### 1. **Convergence Speed Across Optimizers**

This diagram compares the number of epochs required by different optimizers to reach a target accuracy.

```mermaid
graph LR
    A[Optimizer Comparison] --> B[SGD: 50 epochs]
    A --> C[Adam: 35 epochs]
    A --> D[AdamW: 30 epochs]
    A --> E[NeuroOptima: 20 epochs]
```



### 2. **Final Validation Accuracy**

This diagram showcases the final validation accuracy achieved by each optimizer.

```mermaid
graph TD
    A[Final Validation Accuracy]
    A --> B[SGD: 85%]
    A --> C[Adam: 88%]
    A --> D[AdamW: 89%]
    A --> E[NeuroOptima: 92%]
```



### 3. **Training Stability Over Epochs**

This diagram illustrates the stability of training, measured by the variance in loss over epochs.

```mermaid
graph TD
    A[Training Stability]
    A --> B[SGD: High Variance]
    A --> C[Adam: Moderate Variance]
    A --> D[AdamW: Low Variance]
    A --> E[NeuroOptima: Very Low Variance]
```



---

## üìà Interpretation

* **Convergence Speed:** NeuroOptima reaches the target accuracy in fewer epochs compared to traditional optimizers, indicating faster convergence.

* **Validation Accuracy:** The higher final validation accuracy suggests that NeuroOptima generalizes better to unseen data.

* **Training Stability:** Lower variance in training loss indicates more stable and consistent training with NeuroOptima.


## üöÄ Key Features

* **Sharpness-Aware Minimization (SAM):** Enhances model generalization by considering the sharpness of the loss landscape during optimization.

* **Lookahead Mechanism:** Stabilizes training by periodically synchronizing fast and slow weights, leading to more robust convergence.

* **Lion Optimization:** Utilizes sign-based updates for efficient and memory-friendly optimization, particularly beneficial for large-scale models.

* **Adan Momentum:** Incorporates adaptive Nesterov momentum, combining first and second-order gradient information for accelerated and stable convergence.

## üì¶ Installation

Ensure you have PyTorch installed. Then, clone the repository:

```bash
git clone https://github.com/NeuroGhost/NeuroOptima.git
```



Navigate to the directory and install the package:

```bash
cd NeuroOptima
pip install .
```



## üõ†Ô∏è Usage

Integrate **NeuroOptima** into your PyTorch training loop as follows:

```python
from neurooptima import NeuroOptima

model = YourModel()
optimizer = NeuroOptima(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01,
    sam_rho=0.05,
    lookahead_k=5,
    lookahead_alpha=0.5,
    betas=(0.9, 0.999),
    eps=1e-8
)

for input, target in data_loader:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss

    loss = optimizer.step(closure)
```



## üìà Benchmarking

In our experiments on standard datasets such as CIFAR-10 and ImageNet, **NeuroOptima** demonstrated:

* Faster convergence compared to traditional optimizers like Adam and SGD.

* Improved generalization, achieving higher validation accuracy.

* Enhanced stability during training, reducing the occurrence of exploding or vanishing gradients.

*Note: Detailed benchmarking results and scripts are available in the `benchmarks/` directory.*

## üìö References

**NeuroOptima** draws inspiration from the following research works:

* Foret, P., et al. "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.

* Zhang, M., et al. "Lookahead Optimizer: k steps forward, 1 step back." *NeurIPS 2019*.

* Chen, X., et al. "Symbolic Discovery of Optimization Algorithms." *arXiv preprint arXiv:2302.06675*, 2023.

* Xie, L., et al. "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models." *arXiv preprint arXiv:2208.06677*, 2022.
# 
#  
# 
# 
#  
# üß† NeuroOptima Optimizer
# 
# Ru version of readme 
#
**NeuroOptima** ‚Äî —ç—Ç–æ –ø–µ—Ä–µ–¥–æ–≤–æ–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –°–æ–∑–¥–∞–Ω–Ω—ã–π **NeuroGhost**, —ç—Ç–æ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é.

---

## üöÄ –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

* **Sharpness-Aware Minimization (SAM):** –£–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, —É—á–∏—Ç—ã–≤–∞—è —Ä–µ–∑–∫–æ—Å—Ç—å –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤–æ –≤—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

* **–ú–µ—Ö–∞–Ω–∏–∑–º Lookahead:** –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—è –±—ã—Å—Ç—Ä—ã–µ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –≤–µ—Å–∞, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

* **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Lion:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–∫–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∏ —ç–∫–æ–Ω–æ–º–Ω–æ–π –ø–æ –ø–∞–º—è—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ–π –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞.

* **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–æ–º–µ–Ω—Ç Adan:** –í–∫–ª—é—á–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–æ–º–µ–Ω—Ç –ù–µ—Å—Ç–µ—Ä–æ–≤–∞, —Å–æ—á–µ—Ç–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

---

## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω PyTorch. –ó–∞—Ç–µ–º –∫–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:

```bash
git clone https://github.com/NeuroGhost/NeuroOptima.git
```



–ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –∫–∞—Ç–∞–ª–æ–≥ –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç:

```bash
cd NeuroOptima
pip install .
```



---

## üõ†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ **NeuroOptima** –≤ –≤–∞—à —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è PyTorch —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:

```python
from neurooptima import NeuroOptima

model = YourModel()
optimizer = NeuroOptima(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01,
    sam_rho=0.05,
    lookahead_k=5,
    lookahead_alpha=0.5,
    betas=(0.9, 0.999),
    eps=1e-8
)

for input, target in data_loader:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss

    loss = optimizer.step(closure)
```



---

## üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### 1. **–°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤**

```mermaid
graph LR
    A[–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤] --> B[SGD: 50 —ç–ø–æ—Ö]
    A --> C[Adam: 35 —ç–ø–æ—Ö]
    A --> D[AdamW: 30 —ç–ø–æ—Ö]
    A --> E[NeuroOptima: 20 —ç–ø–æ—Ö]
```



### 2. **–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏**

```mermaid
graph TD
    A[–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏]
    A --> B[SGD: 85%]
    A --> C[Adam: 88%]
    A --> D[AdamW: 89%]
    A --> E[NeuroOptima: 92%]
```



### 3. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ —ç–ø–æ—Ö–∞–º**

```mermaid
graph TD
    A[–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è]
    A --> B[SGD: –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è]
    A --> C[Adam: –°—Ä–µ–¥–Ω—è—è –¥–∏—Å–ø–µ—Ä—Å–∏—è]
    A --> D[AdamW: –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è]
    A --> E[NeuroOptima: –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è]
```



---

## üìà –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

* **–°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:** NeuroOptima –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∑–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏.

* **–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:** –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ª—É—á—à—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å NeuroOptima –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

* **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:** –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ—Ç–µ—Ä—å –≥–æ–≤–æ—Ä–∏—Ç –æ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ NeuroOptima.

---

## üìà –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥

–í –º–æ–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ CIFAR-10 –∏ ImageNet, **NeuroOptima** –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª:

* –ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Adam –∏ SGD.

* –£–ª—É—á—à–µ–Ω–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é, –¥–æ—Å—Ç–∏–≥–∞—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

* –ü–æ–≤—ã—à–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —Å–Ω–∏–∂–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –∏–ª–∏ –∏—Å—á–µ–∑–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.


---

## üìö –°—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

**NeuroOptima** –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω —Å–ª–µ–¥—É—é—â–∏–º–∏ –Ω–∞—É—á–Ω—ã–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏:

* Foret, P., et al. "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.

* Zhang, M., et al. "Lookahead Optimizer: k steps forward, 1 step back." *NeurIPS 2019*.

* Chen, X., et al. "Symbolic Discovery of Optimization Algorithms." *arXiv preprint arXiv:2302.06675*, 2023.

* Xie, L., et al. "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models." *arXiv preprint arXiv:2208.06677*, 2022.

---

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è –ª—é–±—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π! –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∏–¥–µ–∏ –∏–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤–Ω–µ—Å—Ç–∏ —Å–≤–æ–π –≤–∫–ª–∞–¥, –Ω–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –æ—Ç–∫—Ä—ã–≤–∞—Ç—å issue –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å pull request.

---

## ü§ù Contributing

Contributions are welcome! If you have suggestions for improvements or want to add new features, feel free to open an issue or submit a pull request.

## üìÑ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

