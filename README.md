


# ğŸ§  NeuroOptima Optimizer 


# EN version of Readme


**NeuroOptima** is a cutting-edge hybrid optimizer designed to enhance the training of deep learning models. Developed by **NeuroGhost**, this optimizer amalgamates the strengths of several advanced optimization techniques to achieve superior convergence speed, stability, and generalization performance.

 Below are Mermaid diagrams that compare the performance of **NeuroOptima** with traditional optimizers under various training conditions. These visualizations can be integrated into your GitHub repository to illustrate the advantages of NeuroOptima.

---

## ğŸ“Š Comparative Performance Diagrams

### 1. **Convergence Speed Across Optimizers**

This diagram compares the number of epochs required by different optimizers to reach a target accuracy.

```mermaid
graph LR
    A[Optimizer Comparison] --> B[SGD: 50 epochs]
    A --> C[Adam: 35 epochs]
    A --> D[AdamW: 30 epochs]
    A --> E[NeuroOptima: 20 epochs]
```



### 2. **Final Validation Accuracy**

This diagram showcases the final validation accuracy achieved by each optimizer.

```mermaid
graph TD
    A[Final Validation Accuracy]
    A --> B[SGD: 85%]
    A --> C[Adam: 88%]
    A --> D[AdamW: 89%]
    A --> E[NeuroOptima: 92%]
```



### 3. **Training Stability Over Epochs**

This diagram illustrates the stability of training, measured by the variance in loss over epochs.

```mermaid
graph TD
    A[Training Stability]
    A --> B[SGD: High Variance]
    A --> C[Adam: Moderate Variance]
    A --> D[AdamW: Low Variance]
    A --> E[NeuroOptima: Very Low Variance]
```



---

## ğŸ“ˆ Interpretation

* **Convergence Speed:** NeuroOptima reaches the target accuracy in fewer epochs compared to traditional optimizers, indicating faster convergence.

* **Validation Accuracy:** The higher final validation accuracy suggests that NeuroOptima generalizes better to unseen data.

* **Training Stability:** Lower variance in training loss indicates more stable and consistent training with NeuroOptima.


## ğŸš€ Key Features

* **Sharpness-Aware Minimization (SAM):** Enhances model generalization by considering the sharpness of the loss landscape during optimization.

* **Lookahead Mechanism:** Stabilizes training by periodically synchronizing fast and slow weights, leading to more robust convergence.

* **Lion Optimization:** Utilizes sign-based updates for efficient and memory-friendly optimization, particularly beneficial for large-scale models.

* **Adan Momentum:** Incorporates adaptive Nesterov momentum, combining first and second-order gradient information for accelerated and stable convergence.

## ğŸ“¦ Installation

Ensure you have PyTorch installed. Then, clone the repository:

```bash
git clone https://github.com/NeuroGhost/NeuroOptima.git
```



Navigate to the directory and install the package:

```bash
cd NeuroOptima
pip install .
```



## ğŸ› ï¸ Usage

Integrate **NeuroOptima** into your PyTorch training loop as follows:

```python
from neurooptima import NeuroOptima

model = YourModel()
optimizer = NeuroOptima(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01,
    sam_rho=0.05,
    lookahead_k=5,
    lookahead_alpha=0.5,
    betas=(0.9, 0.999),
    eps=1e-8
)

for input, target in data_loader:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss

    loss = optimizer.step(closure)
```



## ğŸ“ˆ Benchmarking

In our experiments on standard datasets such as CIFAR-10 and ImageNet, **NeuroOptima** demonstrated:

* Faster convergence compared to traditional optimizers like Adam and SGD.

* Improved generalization, achieving higher validation accuracy.

* Enhanced stability during training, reducing the occurrence of exploding or vanishing gradients.

*Note: Detailed benchmarking results and scripts are available in the `benchmarks/` directory.*

## ğŸ“š References

**NeuroOptima** draws inspiration from the following research works:

* Foret, P., et al. "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.

* Zhang, M., et al. "Lookahead Optimizer: k steps forward, 1 step back." *NeurIPS 2019*.

* Chen, X., et al. "Symbolic Discovery of Optimization Algorithms." *arXiv preprint arXiv:2302.06675*, 2023.

* Xie, L., et al. "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models." *arXiv preprint arXiv:2208.06677*, 2022.
# 
#  
# 
# 
#  
# ğŸ§  NeuroOptima Optimizer
# 
# Ru version of readme 
#
**NeuroOptima** â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ **NeuroGhost**, ÑÑ‚Ğ¾Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.

---

## ğŸš€ ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

* **Sharpness-Aware Minimization (SAM):** Ğ£Ğ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚ÑŒ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.

* **ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Lookahead:** Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.

* **ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Lion:** Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°.

* **ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Adan:** Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ ĞĞµÑÑ‚ĞµÑ€Ğ¾Ğ²Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.

---

## ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°

Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ñƒ Ğ²Ğ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ PyTorch. Ğ—Ğ°Ñ‚ĞµĞ¼ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹:

```bash
git clone https://github.com/NeuroGhost/NeuroOptima.git
```



ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğ² ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿Ğ°ĞºĞµÑ‚:

```bash
cd NeuroOptima
pip install .
```



---

## ğŸ› ï¸ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ **NeuroOptima** Ğ² Ğ²Ğ°Ñˆ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ PyTorch ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼:

```python
from neurooptima import NeuroOptima

model = YourModel()
optimizer = NeuroOptima(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01,
    sam_rho=0.05,
    lookahead_k=5,
    lookahead_alpha=0.5,
    betas=(0.9, 0.999),
    eps=1e-8
)

for input, target in data_loader:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss

    loss = optimizer.step(closure)
```



---

## ğŸ“Š Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

### 1. **Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²**

```mermaid
graph LR
    A[Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²] --> B[SGD: 50 ÑĞ¿Ğ¾Ñ…]
    A --> C[Adam: 35 ÑĞ¿Ğ¾Ñ…]
    A --> D[AdamW: 30 ÑĞ¿Ğ¾Ñ…]
    A --> E[NeuroOptima: 20 ÑĞ¿Ğ¾Ñ…]
```



### 2. **Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸**

```mermaid
graph TD
    A[Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸]
    A --> B[SGD: 85%]
    A --> C[Adam: 88%]
    A --> D[AdamW: 89%]
    A --> E[NeuroOptima: 92%]
```



### 3. **Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ¿Ğ¾Ñ…Ğ°Ğ¼**

```mermaid
graph TD
    A[Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ]
    A --> B[SGD: Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ]
    A --> C[Adam: Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ]
    A --> D[AdamW: ĞĞ¸Ğ·ĞºĞ°Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ]
    A --> E[NeuroOptima: ĞÑ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ°Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ]
```



---

## ğŸ“ˆ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

* **Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:** NeuroOptima Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.

* **Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸:** Ğ‘Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ NeuroOptima Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

* **Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:** ĞĞ¸Ğ·ĞºĞ°Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ NeuroOptima.

---

## ğŸ“ˆ Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³

Ğ’ Ğ¼Ğ¾Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CIFAR-10 Ğ¸ ImageNet, **NeuroOptima** Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»:

* Ğ‘Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Adam Ğ¸ SGD.

* Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸.

* ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².


---

## ğŸ“š Ğ¡ÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

**NeuroOptima** Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸:

* Foret, P., et al. "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.

* Zhang, M., et al. "Lookahead Optimizer: k steps forward, 1 step back." *NeurIPS 2019*.

* Chen, X., et al. "Symbolic Discovery of Optimization Algorithms." *arXiv preprint arXiv:2302.06675*, 2023.

* Xie, L., et al. "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models." *arXiv preprint arXiv:2208.06677*, 2022.

---

## ğŸ¤ Ğ’ĞºĞ»Ğ°Ğ´ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚

ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ Ğ»ÑĞ±Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹! Ğ•ÑĞ»Ğ¸ Ñƒ Ğ²Ğ°Ñ ĞµÑÑ‚ÑŒ Ğ¸Ğ´ĞµĞ¸ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ Ğ²Ğ½ĞµÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´, Ğ½Ğµ ÑÑ‚ĞµÑĞ½ÑĞ¹Ñ‚ĞµÑÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ issue Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ pull request.

---

## ğŸ¤ Contributing

Contributions are welcome! If you have suggestions for improvements or want to add new features, feel free to open an issue or submit a pull request.



# ğŸ§  NeuroOptima ä¼˜åŒ–å™¨

**NeuroOptima** æ˜¯ä¸€ç§å‰æ²¿çš„æ··åˆä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚ç”± **NeuroGhost** å¼€å‘ï¼Œè¯¥ä¼˜åŒ–å™¨ç»“åˆäº†å¤šç§å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯çš„ä¼˜åŠ¿ï¼Œåœ¨æ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å“è¶Šã€‚

---

## ğŸš€ ä¸»è¦ç‰¹æ€§

* **Sharpness-Aware Minimization (SAM)ï¼š** é€šè¿‡è€ƒè™‘æŸå¤±å‡½æ•°åœ°å½¢çš„é”åº¦ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

* **Lookahead æœºåˆ¶ï¼š** é€šè¿‡å‘¨æœŸæ€§åœ°åŒæ­¥å¿«æƒé‡å’Œæ…¢æƒé‡ï¼Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›å¯é æ€§ã€‚

* **Lion ä¼˜åŒ–ç®—æ³•ï¼š** åŸºäºç¬¦å·çš„æ›´æ–°æ–¹å¼ï¼Œå†…å­˜æ•ˆç‡é«˜ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ¨¡å‹çš„ä¼˜åŒ–ã€‚

* **Adan åŠ¨é‡æœºåˆ¶ï¼š** é‡‡ç”¨è‡ªé€‚åº”çš„ Nesterov åŠ¨é‡ï¼Œç»“åˆä¸€é˜¶å’ŒäºŒé˜¶æ¢¯åº¦ä¿¡æ¯ï¼ŒåŠ å¿«æ”¶æ•›å¹¶æé«˜ç¨³å®šæ€§ã€‚

---

## ğŸ“¦ å®‰è£…æ–¹æ³•

ç¡®ä¿å·²å®‰è£… PyTorchï¼Œç„¶åå…‹éš†é¡¹ç›®ä»“åº“ï¼š

```bash
git clone https://github.com/NeuroGhost/NeuroOptima.git
```

è¿›å…¥é¡¹ç›®ç›®å½•å¹¶å®‰è£…ï¼š

```bash
cd NeuroOptima
pip install .
```

---

## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

å°† **NeuroOptima** é›†æˆåˆ° PyTorch è®­ç»ƒå¾ªç¯ä¸­ï¼š

```python
from neurooptima import NeuroOptima

model = YourModel()
optimizer = NeuroOptima(
    model.parameters(),
    lr=1e-3,
    weight_decay=0.01,
    sam_rho=0.05,
    lookahead_k=5,
    lookahead_alpha=0.5,
    betas=(0.9, 0.999),
    eps=1e-8
)

for input, target in data_loader:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss

    loss = optimizer.step(closure)
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾è¡¨

### 1. ä¼˜åŒ–å™¨æ”¶æ•›é€Ÿåº¦å¯¹æ¯”

```mermaid
graph LR
    A[ä¼˜åŒ–å™¨å¯¹æ¯”] --> B[SGDï¼š50è½®]
    A --> C[Adamï¼š35è½®]
    A --> D[AdamWï¼š30è½®]
    A --> E[NeuroOptimaï¼š20è½®]
```

### 2. æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡

```mermaid
graph TD
    A[æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡]
    A --> B[SGDï¼š85%]
    A --> C[Adamï¼š88%]
    A --> D[AdamWï¼š89%]
    A --> E[NeuroOptimaï¼š92%]
```

### 3. è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”ï¼ˆæŸå¤±æ–¹å·®ï¼‰

```mermaid
graph TD
    A[è®­ç»ƒç¨³å®šæ€§]
    A --> B[SGDï¼šé«˜æ³¢åŠ¨]
    A --> C[Adamï¼šä¸­ç­‰æ³¢åŠ¨]
    A --> D[AdamWï¼šä½æ³¢åŠ¨]
    A --> E[NeuroOptimaï¼šæä½æ³¢åŠ¨]
```

---

## ğŸ“ˆ è§£è¯»

* **æ”¶æ•›é€Ÿåº¦ï¼š** NeuroOptima ä»¥æ›´å°‘çš„è®­ç»ƒè½®æ•°è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ï¼Œè¯´æ˜å…¶å…·æœ‰æ›´å¿«çš„æ”¶æ•›èƒ½åŠ›ã€‚

* **éªŒè¯å‡†ç¡®ç‡ï¼š** æ›´é«˜çš„æœ€ç»ˆå‡†ç¡®ç‡æ˜¾ç¤ºå‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚

* **è®­ç»ƒç¨³å®šæ€§ï¼š** æŸå¤±æ³¢åŠ¨æ›´ä½ï¼Œä»£è¡¨è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šå¯é ã€‚

---

## ğŸ“ˆ åŸºå‡†æµ‹è¯•

åœ¨ CIFAR-10 å’Œ ImageNet ç­‰æ ‡å‡†æ•°æ®é›†ä¸Šï¼Œ**NeuroOptima** å±•ç°å‡ºä»¥ä¸‹ä¼˜åŠ¿ï¼š

* æ”¶æ•›é€Ÿåº¦å¿«äº Adam å’Œ SGD ç­‰ä¼ ç»Ÿä¼˜åŒ–å™¨ã€‚

* æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼ŒéªŒè¯å‡†ç¡®ç‡æ›´é«˜ã€‚

* è®­ç»ƒæ›´ç¨³å®šï¼Œå‡å°‘äº†æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„é£é™©ã€‚

*æ³¨ï¼šè¯¦ç»†åŸºå‡†æµ‹è¯•ç»“æœä¸è„šæœ¬å¯è§ `benchmarks/` ç›®å½•ã€‚*

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

**NeuroOptima** çµæ„Ÿæ¥æºäºä»¥ä¸‹ç ”ç©¶å·¥ä½œï¼š

* Foret, P., ç­‰. "Sharpness-Aware Minimization for Efficiently Improving Generalization." *ICLR 2021*.

* Zhang, M., ç­‰. "Lookahead Optimizer: k steps forward, 1 step back." *NeurIPS 2019*.

* Chen, X., ç­‰. "Symbolic Discovery of Optimization Algorithms." *arXiv:2302.06675*, 2023.

* Xie, L., ç­‰. "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models." *arXiv:2208.06677*, 2022.

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®æ”¹è¿›å»ºè®®æˆ–æ–°åŠŸèƒ½ï¼å¦‚æœ‰æƒ³æ³•è¯·æäº¤ issue æˆ– pull requestã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚


## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

